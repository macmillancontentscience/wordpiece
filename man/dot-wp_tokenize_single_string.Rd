% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenization.R
\name{.wp_tokenize_single_string}
\alias{.wp_tokenize_single_string}
\title{Tokenize an Input Word-by-word}
\usage{
.wp_tokenize_single_string(words, vocab, unk_token, max_chars)
}
\arguments{
\item{words}{Character; a vector of words (generated by space-tokenizing a
single input).}

\item{vocab}{Character vector of vocabulary tokens. The tokens are assumed to
be in order of index, with the first index taken as zero to be compatible
with Python implementations.}

\item{unk_token}{Token to represent unknown words.}

\item{max_chars}{Maximum length of word recognized.}
}
\value{
A named integer vector of tokenized words.
}
\description{
Tokenize an Input Word-by-word
}
\keyword{internal}
